## 目录

- [TCP 何时关闭连接](#TCP-何时关闭连接)
- [SO_KEEPALIVE VS 应用层心跳包机制](#SO_KEEPALIVE-VS-应用层心跳包机制)
- [TCP Keepalive 工作原理](#TCP-Keepalive-工作原理)


## TCP 何时关闭连接

shutdown(both) 还需要 close，不然你就泄漏了一个或几个 handle 或 fd 以及相关资源。

read 返回 0 表示你收到了对方发来的 fin，这可能是对方调用 shutdown(send)，也可能是对方调用了 close()。从 tcp 协议本身来说，你是无法知道对方到底是调用了 close() 还是调用了 shutdown(send) 的，os 的 tcp 协议栈也不知道。因此此时是否要 close 取决于你的应用。通常来说如果对方调用的是 close，那么你也可以close。否则你不能 close，例如对方发送一个包给你并 shutdown(write)，然后调用 recv，这时候你还可以返回一个或多个包，连接此时处于半关闭状态，可以一直持续。

这么做的客户端不多（connect -> send -> shtudown(send) -> recv()），但的确有，而且是完全合法的。如果通讯双方都是你自己的代码，那么你知道是哪种情况。如果你不能了解对方的代码，甚至你是个 proxy（代理），两边代码你都不了解，那么通常来说你不能 close。

很多 server/proxy 的实现为当 read 返回 0 就 close，这种实现是错误的，这个实现无法兼容刚才我说的那种情况。

对于 proxy 来说，正确的做法是透传双方的行为。因此，当你 read(client_side_socket) 返回 0 时，你应该对另外一端调用 shutdown(server_side_socket, send)，这样服务器就会 read 返回 0，你透明的传递了这个行为。那么作为 proxy，你什么时候才能 close 呢？client_socket 和 server_socket 上 read 都返回了 0，或者有任何一方返回了 -1 时你可以 close。当然你也可以考虑设置一个超时时间，如果线路上超过 5 分钟没有数据你就断开，但这是另一个维度的问题。

关于 close，要注意的是默认情况下它是一个异步的过程。作为 proxy 来说，如果你想避免大量 close_wait，那么你可以在 close 之前 shutdown，然后启动一个 5s 的 delaytask，在 delaytask 里设置超时时间为 0 的 so_linger，然后 close，对 socket 进行 hard close。这时候 close 是同步的，如果此时不能优雅关闭，那么系统会立刻强制关闭。如果你在 2 处 close 同一个 socket，这是高危行为。因为 2 次 close 之间很可能会有一个新 socket 产生并且值和你第一次 close 的那个一样。你第二次 close 就会错误的 close 了一个不属于你的 socket。这种错误非常难查，因此绝对不要干这种事。

参考：

- <https://www.zhihu.com/question/48871684/answer/113135138>

## SO_KEEPALIVE VS 应用层心跳包机制

TCP Keepalive 和应用层 HeartBeat 优缺点，

**TCP 协议的 Keepalive**

优点：

1. 系统内核完全替上层应用自动给做好了，内核层面计时器相比上层应用，更为高效。上层应用只需要处理数据收发、连接异常通知即可。
2. 使用起来简单，减少了应用层代码的复杂度。也会更节省流量，因为应用层的数据传输到 TCP 协议层时都会被加上额外的包头包尾。由 TCP 协议提供的检活，其发的探测包，理论上实现的会更精妙，耗费更少的流量。

缺点:

1. keepalive 只能检测连接存活，而不能检测连接可用，比如某台服务器因为某些原因导致负载超高，CPU 满了,无法响应任何业务请求，但是使用 TCP 探针则仍旧能够确定连接状态，这就是典型的连接活着但业务提供方已死的状态。对客户端而言，这时的最好选择就是断线后重新连接其他服务器，而不是一直认为当前服务器是可用状态，一直向当前服务器发送些必然会失败的请求。
2. 如果 tcp 连接的另一端突然掉线，这个时候我们并不知道网络已经关闭。而此时，如果有发送数据失败，tcp 会自动进行重传。重传包的优先级高于 keepalive 的包，那就意味着，我们的 keepalive 总是不能发送出去。 而此时，我们也并不知道该连接已经出错而中断。在较长时间的重传失败之后，我们才会知道。

**应用层 HeartBeat**

优点:

1. 有着更大的灵活性，可以控制检测时机，间隔和处理流程，甚至可以在心跳包上附带额外信息，最重要的是可以做到没有上面所说的缺点，不光可以检测连接存在，还可以检测连接可用。
2. 通用，应用层的心跳不依赖协议。如果有一天不用 TCP 要改为 UDP 了，协议层不提供心跳机制了，但是你应用层的心跳依旧是通用的，可能只需要做少许改动就可以继续使用。

缺点:

需要自己实现，增加开发工作量，由于应用特定的网络框架，还可能增加代码结构的复杂度，应用层心跳的流量消耗还是更大的，毕竟这本质上还是个普通的数据包。

## TCP Keepalive 工作原理

TCP Keepalive工作原理
当一个 TCP 连接建立之后，启用 TCP Keepalive 的一端便会启动一个计时器，当这个计时器数值到达 0 之后（也就是经过tcp_keep-alive_time时间后，这个参数之后会讲到），一个 TCP 探测包便会被发出。这个 TCP 探测包是一个纯 ACK 包（规范建议，不应该包含任何数据，但也可以包含1个无意义的字节，比如0x0。），其 Seq号 与上一个包是重复的，所以其实探测保活报文不在窗口控制范围内。

如果一个给定的连接在两小时内（默认时长）没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一：

1.     客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。

2.     客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。

3.     客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。

4.     客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探测的响应。

对于linux内核来说，应用程序若想使用TCP Keepalive，需要设置SO_KEEPALIVE套接字选项才能生效。

有三个重要的参数：

1.     tcp_keepalive_time，在TCP保活打开的情况下，最后一次数据交换到TCP发送第一个保活探测包的间隔，即允许的持续空闲时长，或者说每次正常发送心跳的周期，默认值为7200s（2h）。

2.     tcp_keepalive_probes 在tcp_keepalive_time之后，没有接收到对方确认，继续发送保活探测包次数，默认值为9（次）

3.     tcp_keepalive_intvl，在tcp_keepalive_time之后，没有接收到对方确认，继续发送保活探测包的发送频率，默认值为75s。

其他编程语言有相应的设置方法，这里只谈linux内核参数的配置。例如C语言中的setsockopt()函数，java的Netty服务器框架中也提供了相关接口。
